https://blog.csdn.net/qq_36332660/article/details/131061155

# 分布式训练，微调的学习

## 当前设备，以及各自ip 端口为

|     | 映射ip| 映射端口| 内网ip| GPU|
|----------:|----------:|----------:|----------:|----------:|
|1|210.30.97.178| 2222|  192.168.1.170|NVIDIA A100-SXM4-80GB * 1|
|2|210.30.97.179| 2222|  192.168.1.144|NVIDIA A100-SXM4-80GB * 2|
|3|210.30.97.181| 2222|  192.168.2.183|NVIDIA A100-SXM4-80GB * 2|


命令
torchrun --nproc_per_node=2 --nnodes=2 --node_rank=0 --master_addr=192.168.1.144 --master_port=8000 train.py --train_args_file train_args/sft/qlora/llama2-7b-sft-qlora.json

–nnodes=2：指定总共的节点数，这里是2个。意味着总共有2个机器参与分布式训练。
–node_rank=0：指定当前节点（机器）的排名，这里是0。排名从0开始，用于在分布式环境中区分不同的节点。
–master_addr=192.168.1.144：指定主节点的IP地址，这里是192.168.1.144。主节点用于协调分布式训练过程。
–master_port=8000：指定主节点的端口号，这里是8000。主节点使用指定的端口来与其他节点进行通信。


- 两种分布式训练方式
⚠️：Pytorch 分布式目前只支持 Linux。实现程序并行主要有 DataParallel 和 DistributedDataParallel 两种方式：

DataParallel (DP)：实现简单，代码量较少，启动速度快一点。但速度较慢，且存在负载不均衡的问题。单进程，多线程。主卡显存占用比其他卡会多很多。不支持 Apex 的混合精度训练。是Pytorch官方很久之前给的一种方案。受 Python GIL 的限制，DP的操作原理是将一个batchsize的输入数据均分到多个GPU上分别计算（此处注意，batchsize要大于GPU个数才能划分）。

DistributedDataParallel (DDP)：All-Reduce模式，本意是用来分布式训练（多机多卡），但是也可用于单机多卡。配置稍复杂。多进程。数据分配较均衡。是新一代的多卡训练方法。使用 torch.distributed 库实现并行。torch.distributed 库提供分布式支持，包括 GPU 和 CPU 的分布式训练支持，该库提供了一种类似 MPI 的接口，用于跨多机器网络交换张量数据。它支持几种不同的后端和初始化方法。DDP通过Ring-Reduce的数据交换方法提高了通讯效率，并通过启动多个进程的方式减轻Python GIL的限制，从而提高训练速度。

当然是研究后者


关键技术：
https://blog.csdn.net/qq_14839543/article/details/126378827